{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import logging\n",
    "import rawutil\n",
    "import struct\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "# from typing import Dict, Any\n",
    "import time\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "from multiprocessing import Process, Queue\n",
    "from typing import TypeVar, List, Dict, Tuple, Any\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_ = 0\n",
    "data = {}\n",
    "data['context'] = {'daisy': False,\n",
    "                   'boardmode': 'default',\n",
    "                   'montage': {i: ch for i, ch in enumerate('Fp1,Fp2,T3,C3,C4,T4,O1,O2'.split(','))},\n",
    "                   'connection': 'wifi',\n",
    "                   'gain': [24, 24, 24, 24, 24, 24, 24, 24]\n",
    "                   }\n",
    "\n",
    "\n",
    "data['context']['created'] = datetime.now().timestamp()\n",
    "\n",
    "def aux_(v): return list(struct.pack(\n",
    "    '>hhh', *(np.array([v / 3] * 3) * (16 / 0.002)).astype(int).tolist()))\n",
    "\n",
    "def eeg_(v): return list(rawutil.pack('>u', -v // 24)) * 8\n",
    "def t0(): return ((time.time() * 10) // 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330000"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if (time.time() // 1) % 2:\n",
    "    aux = aux_(1)\n",
    "    eeg = eeg_(1)\n",
    "else:\n",
    "    aux = aux_(-1)\n",
    "    eeg = eeg_(-1)\n",
    "\n",
    "data['context']['created'] = datetime.now().timestamp()\n",
    "data['data'] = [0xa0,  # header\n",
    "                id_ % 256,  # ID 0-255\n",
    "                *eeg,\n",
    "                *aux,\n",
    "                0xc0,  # footer\n",
    "                ] * 10000\n",
    "\n",
    "data['data'] = bytes(data['data'])\n",
    "len(data['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################################################################\n",
    "class Deserialize:\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    def __init__(self):\n",
    "        \"\"\"Constructor\"\"\"\n",
    "\n",
    "        # self.deserialize(data, context)\n",
    "\n",
    "        self._last_marker = 0\n",
    "        self.counter = 0\n",
    "\n",
    "        self.remnant = b''\n",
    "        self.offset = None, None\n",
    "        self._last_aux_shape = 0\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    def deserialize(self, data: np.ndarray, context: Dict[str, Any]) -> None:\n",
    "        \"\"\"From signed 24-bits integer to signed 32-bits integer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data\n",
    "            Numpy array of shape (`33, LENGTH`)\n",
    "        context\n",
    "            Information from the acquisition side useful for deserializing and\n",
    "            that will be packaged back in the stream.\n",
    "        \"\"\"\n",
    "        \n",
    "        # EGG\n",
    "        eeg_data = data[:, 2:26]\n",
    "        eeg_data = getattr(self, f'deserialize_eeg_{context[\"connection\"]}')(\n",
    "            eeg_data, data[:, 1], context)\n",
    "        \n",
    "\n",
    "        # Auxiliar\n",
    "        # stop_byte = data[0][-1]\n",
    "        stop_byte = int((np.median(data[:, -1])))\n",
    "        \n",
    "        aux = self.deserialize_aux(stop_byte, data[:, 26:32], context)\n",
    "        self._last_aux_shape = aux.shape\n",
    "        \n",
    "\n",
    "        # Stream\n",
    "        channels = list(context['montage'].keys())\n",
    "        return eeg_data.T[channels], aux.T\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "\n",
    "    def deserialize_eeg_wifi(self, eeg: np.ndarray, ids: np.ndarray, context: Dict[str, Any]) -> np.ndarray:\n",
    "        \"\"\"From signed 24-bits integer to signed 32-bits integer by channels.\n",
    "\n",
    "        The `Cyton data format <https://docs.openbci.com/docs/02Cyton/CytonDataFormat>`_\n",
    "        says that only can send packages of 33 bits, when a Daisy board is\n",
    "        attached these same packages will be sent at double speed in favor to\n",
    "        keep the desired sample rate for 16 channels.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        eeg\n",
    "            Numpy array in signed 24-bits integer (`8, LENGTH`)\n",
    "        ids\n",
    "            List of IDs for eeg data.\n",
    "        context\n",
    "            Information from the acquisition side useful for deserializing and\n",
    "            that will be packaged back in the stream.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        eeg_data\n",
    "            EEG data in microvolts, signed 32-bits integer, (`CHANNELS, LENGTH`),\n",
    "            if there is a Daisy board `CHANNELS` is 16, otherwise is 8.\n",
    "        \"\"\"\n",
    "        t0 = time.time()\n",
    "        \n",
    "        print(eeg)\n",
    "        eeg_data = np.array([[rawutil.unpack('>u', bytes(ch))[0] for ch in row.reshape(-1, 3).tolist()] for row in eeg])\n",
    "        \n",
    "        \n",
    "        print((time.time()-t0)*1000)\n",
    "        \n",
    "        \n",
    "        eeg_data = eeg_data * self.scale_factor_eeg\n",
    "\n",
    "        if context['daisy']:\n",
    "\n",
    "            # # If offset, the pair index condition must change\n",
    "            if np.array(self.offset[0]).any():\n",
    "                eeg_data = np.concatenate([[self.offset[0]], eeg_data], axis=0)\n",
    "                ids = np.concatenate([[self.offset[1]], ids], axis=0)\n",
    "                # pair = not pair\n",
    "\n",
    "            if ids[0] != ids[1]:\n",
    "                eeg_data = np.delete(eeg_data, 0, axis=0)\n",
    "                ids = np.delete(ids, 0, axis=0)\n",
    "\n",
    "            # if not pair dataset, create an offeset\n",
    "            if eeg_data.shape[0] % 2:\n",
    "                self.offset = eeg_data[-1], ids[-1]\n",
    "                eeg_data = np.delete(eeg_data, -1, axis=0)\n",
    "                ids = np.delete(ids, -1, axis=0)\n",
    "            else:\n",
    "                self.offset = None, None\n",
    "\n",
    "            return eeg_data.reshape(-1, 16)\n",
    "        \n",
    "        print((time.time()-t0)*1000)\n",
    "        return eeg_data\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    def deserialize_eeg_serial(self, eeg: np.ndarray, ids: np.ndarray, context: Dict[str, Any]) -> np.ndarray:\n",
    "        \"\"\"From signed 24-bits integer to signed 32-bits integer by channels.\n",
    "\n",
    "        The `Cyton data format <https://docs.openbci.com/docs/02Cyton/CytonDataFormat>`_\n",
    "        says that only can send packages of 33 bits, over serial (RFduino) this\n",
    "        limit is absolute, when a Daisy board is attached these same amount of\n",
    "        packages will be sent, in this case, the data must be distributed and\n",
    "        interpolated in order to complete the sample rate.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        eeg\n",
    "            Numpy array in signed 24-bits integer (`8, LENGTH`)\n",
    "        ids\n",
    "            List of IDs for eeg data.\n",
    "        context\n",
    "            Information from the acquisition side useful for deserializing and\n",
    "            that will be packaged back in the stream.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        eeg_data\n",
    "            EEG data in microvolts, signed 32-bits integer, (`CHANNELS, LENGTH`),\n",
    "            if there is a Daisy board `CHANNELS` is 16, otherwise is 8.\n",
    "        \"\"\"\n",
    "\n",
    "        eeg_data = np.array([[rawutil.unpack('>u', bytes(ch))[0]\n",
    "                              for ch in row.reshape(-1, 3).tolist()] for row in eeg])\n",
    "        eeg_data = eeg_data * self.scale_factor_eeg\n",
    "\n",
    "        if context['daisy']:\n",
    "\n",
    "            even = not ids[0] % 2\n",
    "\n",
    "            # If offset, the even index condition must change\n",
    "            if np.array(self.offset[0]).any():\n",
    "                eeg_data = np.concatenate([[self.offset[0]], eeg_data], axis=0)\n",
    "                ids = np.concatenate([[self.offset[1]], ids], axis=0)\n",
    "                even = not even\n",
    "\n",
    "            # if not even dataset, create an offset\n",
    "            if eeg_data.shape[0] % 2:\n",
    "                self.offset = eeg_data[-1], ids[-1]\n",
    "                eeg_data = np.delete(eeg_data, -1, axis=0)\n",
    "                ids = np.delete(ids, -1, axis=0)\n",
    "\n",
    "            # Data can start with a even or odd id\n",
    "            if even:\n",
    "                board = eeg_data[::2]\n",
    "                daisy = eeg_data[1::2]\n",
    "            else:\n",
    "                daisy = eeg_data[::2]\n",
    "                board = eeg_data[1::2]\n",
    "\n",
    "            board = np.array([np.interp(np.arange(0, p.shape[0], 0.5), np.arange(\n",
    "                p.shape[0]), p) for p in board.T]).T\n",
    "            daisy = np.array([np.interp(np.arange(0, p.shape[0], 0.5), np.arange(\n",
    "                p.shape[0]), p) for p in daisy.T]).T\n",
    "\n",
    "            eeg = np.concatenate([np.stack(board), np.stack(daisy)], axis=1)\n",
    "\n",
    "        else:\n",
    "            eeg = eeg_data\n",
    "\n",
    "        return eeg\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "\n",
    "    @property\n",
    "    def scale_factor_eeg(self) -> float:\n",
    "        \"\"\"Vector with the correct factors for scale eeg data samples.\"\"\"\n",
    "        gain = 24\n",
    "        # vref = 4.5  # for V\n",
    "        vref = 4500000  # for uV\n",
    "\n",
    "        return vref / (gain * ((2 ** 23) - 1))\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "\n",
    "    def deserialize_aux(self, stop_byte: int, aux: int, context: Dict[str, Any]) -> np.ndarray:\n",
    "        \"\"\"Determine the content of `AUX` bytes and format it.\n",
    "\n",
    "        Auxialiar data could contain different kind of information: accelometer,\n",
    "        user defined, time stamped and digital or analog inputs.\n",
    "        The context of `AUX` bytes are determined by the stop byte.\n",
    "\n",
    "        If `stop_byte` is `0xc0` the `AUX` bytes contain `Standard with accel`,\n",
    "        this data are packaged at different frequency, they will be show up each\n",
    "        10 or 11 packages, the final list will contain accelometer value in `G`\n",
    "        units for axis `X`, `Y` and `Z` respectively and `None` when are not\n",
    "        availables.\n",
    "\n",
    "        If `stop_byte` is `0xc1` the `AUX` bytes contain `Standard with raw aux`,\n",
    "        there are 3 types of raw data: `digital` in wich case the final list\n",
    "        will contain the values for `D11`, `D12`, `D13`, `D17`, `D18`; `analog`\n",
    "        with the values for `A7` (`D13`), `A6` (`D12`), `A5` (`D11`); `markers`\n",
    "        data contain the the marker sended with `send_marker()` method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        stop_byte\n",
    "             0xCX where X is 0-F in hex.\n",
    "        aux\n",
    "            6 bytes of data defined and parsed based on the `Footer` bytes.\n",
    "        context\n",
    "            Information from the acquisition side useful for deserializing and\n",
    "            that will be packaged back in the stream.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            Correct data formated.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Standard with accel\n",
    "        if stop_byte == 0xc0:\n",
    "            return 0.002 * \\\n",
    "                np.array([struct.unpack('>hhh', a.astype('i1').tobytes())\n",
    "                          for a in aux]) / 16\n",
    "\n",
    "        # Standard with raw aux\n",
    "        elif stop_byte == 0xc1:\n",
    "\n",
    "            if context['boardmode'] == 'analog':\n",
    "                # A7, A6, A5\n",
    "                # D13, D12, D11\n",
    "                return aux[:, 1::2]\n",
    "\n",
    "            elif context['boardmode'] == 'digital':\n",
    "                # D11, D12, D13, D17, D18\n",
    "                return np.delete(aux, 4, axis=1)\n",
    "\n",
    "            elif context['boardmode'] == 'marker':\n",
    "                # Some time for some reason, marker not always send back from\n",
    "                # OpenBCI, so this module implement a strategy to send a burst of\n",
    "                # markers but read back only one.\n",
    "                a = aux[:, 1]\n",
    "                a[a > ord('Z')] = 0\n",
    "                a[a < ord('A')] = 0\n",
    "                return a\n",
    "\n",
    "        # User defined\n",
    "        elif stop_byte == 0xc2:\n",
    "            pass\n",
    "\n",
    "        # Time stamped set with accel\n",
    "        elif stop_byte == 0xc3:\n",
    "            pass\n",
    "\n",
    "        # Time stamped with accel\n",
    "        elif stop_byte == 0xc4:\n",
    "            pass\n",
    "\n",
    "        # Time stamped set with raw auxcalculate_sample_rate\n",
    "        elif stop_byte == 0xc5:\n",
    "            pass\n",
    "\n",
    "        # Time stamped with raw aux\n",
    "        elif stop_byte == 0xc6:\n",
    "            pass\n",
    "\n",
    "        return np.zeros(self._last_aux_shape)\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    def stream(self, eeg_queue, data, samples, context):\n",
    "        \"\"\"Kafka produser.\n",
    "\n",
    "        Stream data to network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : list\n",
    "            The EEG data format.\n",
    "        samples : int\n",
    "            The number of samples in this package.\n",
    "\n",
    "        \"\"\"\n",
    "        context.update({'samples': samples})\n",
    "        # context['created'] = datetime.now().timestamp()\n",
    "\n",
    "        data_ = {'context': context,\n",
    "                 'data': data,\n",
    "                 # 'binary_created': self.created,\n",
    "                 # 'created': datetime.now().timestamp(),\n",
    "                 # 'samples': samples,\n",
    "                 }\n",
    "\n",
    "        # self.producer_eeg.send('eeg', data_)\n",
    "        eeg_queue.put(data_)\n",
    "\n",
    "        if DEBUG:\n",
    "            logging.info(f\"streamed {samples} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIN_HEADER = 0xa0\n",
    "DEBUG = True\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "def align_data(binary: bytes) -> Tuple[np.ndarray, bytes]:\n",
    "    \"\"\"Align data following the headers and footers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    binary\n",
    "        Data raw from OpenBCI board.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_aligned\n",
    "        Numpy array of shape (`33, LENGTH`) with headers and footer aligned.\n",
    "    remnant\n",
    "        This bytes could be used for complete next binary input.\n",
    "    \"\"\"\n",
    "\n",
    "    data = np.array(list(binary))\n",
    "\n",
    "    # Search for the the first index with a `BIN_HEADER`\n",
    "    start = [np.median(np.roll(data, -i, axis=0)[::33])\n",
    "             == BIN_HEADER for i in range(33)].index(True)\n",
    "\n",
    "    if (start == 0) and (data.shape[0] % 33 == 0):\n",
    "        data_aligned = data\n",
    "        remnant = b''\n",
    "    else:\n",
    "        # Fix the offset to complete 33 bytes divisible array\n",
    "        end = (data.shape[0] - start) % 33\n",
    "        data_aligned = data[start:-end]\n",
    "        remnant = binary[-end:]\n",
    "\n",
    "    data_aligned = data_aligned.reshape(-1, 33)\n",
    "\n",
    "    return data_aligned, remnant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NoBrokersAvailable",
     "evalue": "NoBrokersAvailable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoBrokersAvailable\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f6a234ade0e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m producer_eeg = KafkaProducer(bootstrap_servers=['localhost:9092'],\n\u001b[0m\u001b[1;32m      2\u001b[0m                                   \u001b[0mcompression_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gzip'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                   \u001b[0mvalue_serializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                   )\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/kafka/producer/kafka.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **configs)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         client = KafkaClient(metrics=self._metrics, metric_group_prefix='producer',\n\u001b[0m\u001b[1;32m    382\u001b[0m                              \u001b[0mwakeup_timeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_block_ms'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                              **self.config)\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/kafka/client_async.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **configs)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'api_version'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0mcheck_timeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'api_version_auto_timeout_ms'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'api_version'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_can_bootstrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/kafka/client_async.py\u001b[0m in \u001b[0;36mcheck_version\u001b[0;34m(self, node_id, timeout, strict)\u001b[0m\n\u001b[1;32m    898\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtry_node\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoBrokersAvailable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtry_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtry_node\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoBrokersAvailable\u001b[0m: NoBrokersAvailable"
     ]
    }
   ],
   "source": [
    "producer_eeg = KafkaProducer(bootstrap_servers=['localhost:9092'],\n",
    "                                  compression_type='gzip',\n",
    "                                  value_serializer=pickle.dumps,\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "684.6182346343994\n",
      "685.0900650024414\n",
      "Total: 765.6495571136475\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "data_, _ = align_data(data['data'])\n",
    "dess(data_)\n",
    "\n",
    "t1 = time.time()\n",
    "print('Total:', (t1-t0)*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "des = Deserialize()\n",
    "# def dess(d):\n",
    "#     return des.deserialize(d, data['context'])\n",
    "\n",
    "dess = lambda d:des.deserialize(d, data['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "822.4489688873291\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "data_, _ = align_data(data['data'])\n",
    "N = 1\n",
    "with Pool(N) as pool:\n",
    "    q = pool.map(dess, np.array_split(data_, N))\n",
    "    e, a = zip(*q)\n",
    "#     print(e[0].shape)\n",
    "    \n",
    "np.concatenate(e, axis=1).shape\n",
    "t1 = time.time()\n",
    "print((t1-t0)*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathos.multiprocessing import ProcessingPool as Pool"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
